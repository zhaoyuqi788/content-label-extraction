"""é¢„å¤„ç†æ¨¡å—

è´Ÿè´£æ–‡æœ¬æ¸…æ´—ã€åˆ†æ®µã€æ¥æºæ ‡æ³¨ç­‰é¢„å¤„ç†ä»»åŠ¡ã€‚
"""

import re
import html
from typing import Dict, List, Optional, Tuple
from loguru import logger

from .schemas import ContentInput, SourceType
from .utils import clean_text, load_config


class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–é¢„å¤„ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or load_config()
        
        # é¢„å¤„ç†é…ç½®
        pipeline_config = self.config.get('pipeline', {})
        self.max_chars_per_segment = pipeline_config.get('max_chars_per_segment', 1500)
        self.source_priority = pipeline_config.get('source_priority', ['body', 'title', 'asr', 'ocr'])
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self._compile_patterns()
        
        logger.info("æ–‡æœ¬é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _compile_patterns(self):
        """ç¼–è¯‘å¸¸ç”¨æ­£åˆ™è¡¨è¾¾å¼"""
        # HTMLæ ‡ç­¾
        self.html_pattern = re.compile(r'<[^>]+>')
        
        # å¤šä½™ç©ºç™½
        self.whitespace_pattern = re.compile(r'\s+')
        
        # è¡¨æƒ…ç¬¦å·ï¼ˆä¿ç•™å¸¸è§çš„æ¨èè¡¨æƒ…ï¼‰
        self.emoji_keep_pattern = re.compile(r'[ğŸ‘ğŸ‘ğŸ’•â¤ï¸ğŸ˜ğŸ¥°ğŸ˜ŠğŸ˜­ğŸ˜‚ğŸ”¥ğŸ’¯âœ¨]')
        self.emoji_remove_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251]+')
        
        # URLé“¾æ¥
        self.url_pattern = re.compile(r'https?://[^\s]+|www\.[^\s]+')
        
        # @ç”¨æˆ·å
        self.mention_pattern = re.compile(r'@[\w\u4e00-\u9fff]+')
        
        # #è¯é¢˜æ ‡ç­¾
        self.hashtag_pattern = re.compile(r'#[\w\u4e00-\u9fff]+')
        
        # é‡å¤æ ‡ç‚¹
        self.repeat_punct_pattern = re.compile(r'([ï¼ï¼Ÿã€‚ï¼Œï¼›ï¼š]){2,}')
        
        # ç‰¹æ®Šå­—ç¬¦æ¸…ç†
        self.special_chars_pattern = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]')
    
    def preprocess_content(self, content: ContentInput) -> ContentInput:
        """é¢„å¤„ç†å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹
            
        Returns:
            ContentInput: é¢„å¤„ç†åçš„å†…å®¹
        """
        logger.debug(f"å¼€å§‹é¢„å¤„ç†å†…å®¹: {content.content_id}")
        
        # é¢„å¤„ç†å„ä¸ªå­—æ®µ
        processed_title = self._preprocess_text(content.title, 'title') if content.title else None
        processed_body = self._preprocess_text(content.body, 'body') if content.body else None
        processed_ocr = self._preprocess_text(content.ocr_text, 'ocr') if content.ocr_text else None
        processed_asr = self._preprocess_text(content.asr_text, 'asr') if content.asr_text else None
        
        # åº”ç”¨é•¿åº¦é™åˆ¶
        if processed_body and len(processed_body) > 2000:
            logger.warning(f"å†…å®¹è¿‡é•¿è¢«æˆªæ–­ {content.content_id}: {len(processed_body)} -> 2000")
            processed_body = processed_body[:2000]
        
        # åˆ›å»ºé¢„å¤„ç†åçš„å†…å®¹
        processed_content = ContentInput(
            content_id=content.content_id,
            title=processed_title,
            body=processed_body,
            ocr_text=processed_ocr,
            asr_text=processed_asr,
            extra_fields=content.extra_fields
        )
        
        # éªŒè¯å¤„ç†ç»“æœ
        self._validate_processed_content(processed_content)
        
        logger.debug(f"å®Œæˆé¢„å¤„ç†å†…å®¹: {content.content_id}")
        return processed_content
    
    def _preprocess_text(self, text: str, source_type: str) -> str:
        """é¢„å¤„ç†å•ä¸ªæ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            source_type: æ¥æºç±»å‹
            
        Returns:
            str: é¢„å¤„ç†åçš„æ–‡æœ¬
        """
        if not text or not text.strip():
            return ""
        
        # 1. HTMLè§£ç 
        text = html.unescape(text)
        
        # 2. ç§»é™¤HTMLæ ‡ç­¾
        text = self.html_pattern.sub('', text)
        
        # 3. ç§»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
        text = self.special_chars_pattern.sub('', text)
        
        # 4. å¤„ç†URLå’Œ@ç”¨æˆ·åï¼ˆæ ¹æ®æ¥æºç±»å‹å†³å®šæ˜¯å¦ä¿ç•™ï¼‰
        if source_type in ['title', 'body']:
            # æ­£æ–‡ä¸­çš„é“¾æ¥æ›¿æ¢ä¸ºå ä½ç¬¦
            text = self.url_pattern.sub('[é“¾æ¥]', text)
            text = self.mention_pattern.sub('[ç”¨æˆ·]', text)
        else:
            # OCR/ASRä¸­ç›´æ¥ç§»é™¤
            text = self.url_pattern.sub('', text)
            text = self.mention_pattern.sub('', text)
        
        # 5. å¤„ç†è¯é¢˜æ ‡ç­¾ï¼ˆä¿ç•™ä½†ç®€åŒ–ï¼‰
        text = self.hashtag_pattern.sub(lambda m: m.group(0)[1:], text)  # ç§»é™¤#å·
        
        # 6. å¤„ç†è¡¨æƒ…ç¬¦å·
        text = self._process_emojis(text)
        
        # 7. æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
        text = self._normalize_punctuation(text)
        
        # 8. æ¸…ç†å¤šä½™ç©ºç™½
        text = self.whitespace_pattern.sub(' ', text)
        text = text.strip()
        
        # 9. ç¹ä½“è½¬ç®€ä½“ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.config.get('preprocessing', {}).get('convert_traditional', False):
            text = self._convert_traditional_to_simplified(text)
        
        return text
    
    def _process_emojis(self, text: str) -> str:
        """å¤„ç†è¡¨æƒ…ç¬¦å·
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            str: å¤„ç†åçš„æ–‡æœ¬
        """
        # ä¿ç•™é‡è¦çš„è¡¨æƒ…ç¬¦å·
        important_emojis = self.emoji_keep_pattern.findall(text)
        
        # ç§»é™¤å…¶ä»–è¡¨æƒ…ç¬¦å·
        text = self.emoji_remove_pattern.sub('', text)
        
        # å°†é‡è¦è¡¨æƒ…ç¬¦å·è½¬æ¢ä¸ºæ–‡æœ¬æè¿°
        emoji_map = {
            'ğŸ‘': '[èµ]',
            'ğŸ‘': '[è¸©]', 
            'ğŸ’•': '[çˆ±å¿ƒ]',
            'â¤ï¸': '[çº¢å¿ƒ]',
            'ğŸ˜': '[èŠ±ç—´]',
            'ğŸ¥°': '[å¯çˆ±]',
            'ğŸ˜Š': '[å¾®ç¬‘]',
            'ğŸ˜­': '[å“­]',
            'ğŸ˜‚': '[ç¬‘å“­]',
            'ğŸ”¥': '[ç«]',
            'ğŸ’¯': '[æ»¡åˆ†]',
            'âœ¨': '[é—ªäº®]'
        }
        
        for emoji, desc in emoji_map.items():
            text = text.replace(emoji, desc)
        
        return text
    
    def _normalize_punctuation(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            str: æ ‡å‡†åŒ–åçš„æ–‡æœ¬
        """
        # ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        punctuation_map = {
            'ï¼ï¼+': 'ï¼',
            'ï¼Ÿï¼Ÿ+': 'ï¼Ÿ',
            'ã€‚ã€‚+': 'ã€‚',
            'ï¼Œï¼Œ+': 'ï¼Œ',
            'ï¼›ï¼›+': 'ï¼›',
            'ï¼šï¼š+': 'ï¼š',
            '~~~+': '~',
            '---+': '-'
        }
        
        for pattern, replacement in punctuation_map.items():
            text = re.sub(pattern, replacement, text)
        
        # å¤„ç†é‡å¤æ ‡ç‚¹
        text = self.repeat_punct_pattern.sub(r'\1', text)
        
        # æ ‡å‡†åŒ–å¼•å·
        text = re.sub(r'[""]', '"', text)
        text = re.sub(r'[''']', "'", text)
        
        return text
    
    def _convert_traditional_to_simplified(self, text: str) -> str:
        """ç¹ä½“è½¬ç®€ä½“ï¼ˆç®€å•å®ç°ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            str: è½¬æ¢åçš„æ–‡æœ¬
        """
        # è¿™é‡Œå¯ä»¥é›†æˆæ›´å®Œæ•´çš„ç¹ç®€è½¬æ¢åº“ï¼Œå¦‚ opencc
        # ç›®å‰æä¾›åŸºç¡€çš„å­—ç¬¦æ˜ å°„
        traditional_map = {
            'è­·': 'æŠ¤', 'è†š': 'è‚¤', 'ç”¢': 'äº§', 'å“': 'å“',
            'è³ª': 'è´¨', 'é‡': 'é‡', 'åƒ¹': 'ä»·', 'æ ¼': 'æ ¼',
            'è³¼': 'è´­', 'è²·': 'ä¹°', 'è©•': 'è¯„', 'åƒ¹': 'ä»·',
            'å„ª': 'ä¼˜', 'é»': 'ç‚¹', 'ç¼º': 'ç¼º', 'é»': 'ç‚¹',
            'é©': 'é€‚', 'åˆ': 'åˆ', 'æ•': 'æ•', 'æ„Ÿ': 'æ„Ÿ',
            'ä¹¾': 'å¹²', 'ç‡¥': 'ç‡¥', 'æ²¹': 'æ²¹', 'æ€§': 'æ€§',
            'æ··': 'æ··', 'åˆ': 'åˆ', 'æ€§': 'æ€§', 'è‚Œ': 'è‚Œ',
            'è†š': 'è‚¤', 'è³ª': 'è´¨', 'å•': 'é—®', 'é¡Œ': 'é¢˜',
            'æ•ˆ': 'æ•ˆ', 'æœ': 'æœ', 'æˆ': 'æˆ', 'åˆ†': 'åˆ†',
            'æ¿ƒ': 'æµ“', 'åº¦': 'åº¦', 'è³ª': 'è´¨', 'åœ°': 'åœ°',
            'é¡': 'é¢œ', 'è‰²': 'è‰²', 'å‘³': 'å‘³', 'é“': 'é“'
        }
        
        for trad, simp in traditional_map.items():
            text = text.replace(trad, simp)
        
        return text
    
    def _validate_processed_content(self, content: ContentInput):
        """éªŒè¯é¢„å¤„ç†åçš„å†…å®¹
        
        Args:
            content: é¢„å¤„ç†åçš„å†…å®¹
            
        Raises:
            ValueError: å¦‚æœå†…å®¹æ— æ•ˆ
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ–‡æœ¬
        combined_text = content.get_combined_text()
        if not combined_text.strip():
            raise ValueError(f"é¢„å¤„ç†åæ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬å†…å®¹: {content.content_id}")
        
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
        if len(combined_text) < 10:
            logger.warning(f"é¢„å¤„ç†åæ–‡æœ¬è¿‡çŸ­: {content.content_id}, é•¿åº¦: {len(combined_text)}")
        
        # æ£€æŸ¥å­—ç¬¦ç¼–ç 
        try:
            combined_text.encode('utf-8')
        except UnicodeEncodeError as e:
            raise ValueError(f"æ–‡æœ¬ç¼–ç é”™è¯¯: {content.content_id}, {e}")
    
    def get_source_priority_mapping(self) -> Dict[str, int]:
        """è·å–æ¥æºä¼˜å…ˆçº§æ˜ å°„
        
        Returns:
            Dict[str, int]: æ¥æºåˆ°ä¼˜å…ˆçº§çš„æ˜ å°„
        """
        return {source: i for i, source in enumerate(self.source_priority)}


class PreprocessingPipeline:
    """é¢„å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–é¢„å¤„ç†æµæ°´çº¿
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or load_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_preprocessor = TextPreprocessor(self.config)
        
        logger.info("é¢„å¤„ç†æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
    
    def process(self, contents: List[ContentInput]) -> List[ContentInput]:
        """å¤„ç†å†…å®¹åˆ—è¡¨
        
        Args:
            contents: è¾“å…¥å†…å®¹åˆ—è¡¨
            
        Returns:
            List[ContentInput]: é¢„å¤„ç†åçš„å†…å®¹åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹é¢„å¤„ç† {len(contents)} ä¸ªå†…å®¹")
        
        processed_contents = []
        
        for content in contents:
            try:
                # æ–‡æœ¬é¢„å¤„ç†
                preprocessed_content = self.text_preprocessor.preprocess_content(content)
                processed_contents.append(preprocessed_content)
                
            except Exception as e:
                logger.error(f"é¢„å¤„ç†å¤±è´¥ {content.content_id}: {e}")
                # å¯ä»¥é€‰æ‹©è·³è¿‡æˆ–ä½¿ç”¨åŸå§‹å†…å®¹
                continue
        
        logger.info(f"é¢„å¤„ç†å®Œæˆ: {len(contents)} -> {len(processed_contents)} ä¸ªå†…å®¹")
        return processed_contents
    
    def process_single(self, content: ContentInput) -> ContentInput:
        """å¤„ç†å•ä¸ªå†…å®¹
        
        Args:
            content: è¾“å…¥å†…å®¹
            
        Returns:
            ContentInput: é¢„å¤„ç†åçš„å†…å®¹
        """
        return self.text_preprocessor.preprocess_content(content)


def create_preprocessing_pipeline(config_path: Optional[str] = None) -> PreprocessingPipeline:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºé¢„å¤„ç†æµæ°´çº¿
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        PreprocessingPipeline: é¢„å¤„ç†æµæ°´çº¿å®ä¾‹
    """
    config = load_config(config_path) if config_path else None
    return PreprocessingPipeline(config)


def preprocess_contents(contents: List[ContentInput], 
                       config_path: Optional[str] = None) -> List[ContentInput]:
    """ä¾¿æ·å‡½æ•°ï¼šé¢„å¤„ç†å†…å®¹åˆ—è¡¨
    
    Args:
        contents: å†…å®¹åˆ—è¡¨
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        List[ContentInput]: é¢„å¤„ç†åçš„å†…å®¹åˆ—è¡¨
    """
    pipeline = create_preprocessing_pipeline(config_path)
    return pipeline.process(contents);
