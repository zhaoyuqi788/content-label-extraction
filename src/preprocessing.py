"""é¢„å¤„ç†æ¨¡å—

è´Ÿè´£æ–‡æœ¬æ¸…æ´—ã€åˆ†æ®µã€æ¥æºæ ‡æ³¨ç­‰é¢„å¤„ç†ä»»åŠ¡ã€‚
"""

import re
import html
from typing import Dict, List, Optional, Tuple
from loguru import logger

from .schemas import ContentInput, SourceType
from .utils import clean_text, load_config


class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–é¢„å¤„ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or load_config()
        
        # é¢„å¤„ç†é…ç½®
        pipeline_config = self.config.get('pipeline', {})
        self.max_chars_per_segment = pipeline_config.get('max_chars_per_segment', 1500)
        self.source_priority = pipeline_config.get('source_priority', ['body', 'title', 'asr', 'ocr'])
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self._compile_patterns()
        
        logger.info("æ–‡æœ¬é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _compile_patterns(self):
        """ç¼–è¯‘å¸¸ç”¨æ­£åˆ™è¡¨è¾¾å¼"""
        # HTMLæ ‡ç­¾
        self.html_pattern = re.compile(r'<[^>]+>')
        
        # å¤šä½™ç©ºç™½
        self.whitespace_pattern = re.compile(r'\s+')
        
        # è¡¨æƒ…ç¬¦å·ï¼ˆä¿ç•™å¸¸è§çš„æ¨èè¡¨æƒ…ï¼‰
        self.emoji_keep_pattern = re.compile(r'[ğŸ‘ğŸ‘ğŸ’•â¤ï¸ğŸ˜ğŸ¥°ğŸ˜ŠğŸ˜­ğŸ˜‚ğŸ”¥ğŸ’¯âœ¨]')
        # ä¿®å¤è¡¨æƒ…ç¬¦å·ç§»é™¤æ­£åˆ™ï¼Œé¿å…è¯¯åˆ ä¸­æ–‡å­—ç¬¦
        self.emoji_remove_pattern = re.compile(
            r'[\U0001F600-\U0001F64F]|'  # è¡¨æƒ…ç¬¦å·
            r'[\U0001F300-\U0001F5FF]|'  # æ‚é¡¹ç¬¦å·å’Œè±¡å½¢æ–‡å­—
            r'[\U0001F680-\U0001F6FF]|'  # äº¤é€šå’Œåœ°å›¾ç¬¦å·
            r'[\U0001F1E0-\U0001F1FF]|'  # åŒºåŸŸæŒ‡ç¤ºç¬¦å·
            r'[\U00002702-\U000027B0]|'  # æ‚é¡¹ç¬¦å·
            r'[\U000024C2-\U000024FF]|'  # å°é—­å­—æ¯æ•°å­—ï¼ˆä¿®æ­£èŒƒå›´ï¼‰
            r'[\U0001F100-\U0001F1FF]|'  # å°é—­å­—æ¯æ•°å­—è¡¥å……
            r'[\U0001F200-\U0001F2FF]'   # å°é—­CJKå­—æ¯å’Œæœˆä»½
        )
        
        # URLé“¾æ¥
        self.url_pattern = re.compile(r'https?://[^\s]+|www\.[^\s]+')
        
        # @ç”¨æˆ·å
        self.mention_pattern = re.compile(r'@[\w\u4e00-\u9fff]+')
        
        # #è¯é¢˜æ ‡ç­¾
        self.hashtag_pattern = re.compile(r'#[\w\u4e00-\u9fff]+')
        
        # é‡å¤æ ‡ç‚¹
        self.repeat_punct_pattern = re.compile(r'([ï¼ï¼Ÿã€‚ï¼Œï¼›ï¼š]){2,}')
        
        # ç‰¹æ®Šå­—ç¬¦æ¸…ç†
        self.special_chars_pattern = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]')
    
    def preprocess_content(self, content: ContentInput) -> ContentInput:
        """é¢„å¤„ç†å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹
            
        Returns:
            ContentInput: é¢„å¤„ç†åçš„å†…å®¹
        """
        logger.debug(f"å¼€å§‹é¢„å¤„ç†å†…å®¹: {content.content_id}")
        
        # é¢„å¤„ç†å„ä¸ªå­—æ®µ
        processed_title = self._preprocess_text(content.title, 'title') if content.title else None
        processed_body = self._preprocess_text(content.body, 'body') if content.body else None
        processed_ocr = self._preprocess_text(content.ocr_text, 'ocr') if content.ocr_text else None
        processed_asr = self._preprocess_text(content.asr_text, 'asr') if content.asr_text else None
        
        # åº”ç”¨é•¿åº¦é™åˆ¶
        if processed_body and len(processed_body) > 2000:
            logger.warning(f"å†…å®¹è¿‡é•¿è¢«æˆªæ–­ {content.content_id}: {len(processed_body)} -> 2000")
            processed_body = processed_body[:2000]
        
        # åˆ›å»ºé¢„å¤„ç†åçš„å†…å®¹
        processed_content = ContentInput(
            content_id=content.content_id,
            title=processed_title,
            body=processed_body,
            ocr_text=processed_ocr,
            asr_text=processed_asr,
            extra_fields=content.extra_fields
        )
        
        # éªŒè¯å¤„ç†ç»“æœ
        self._validate_processed_content(processed_content)
        
        logger.debug(f"å®Œæˆé¢„å¤„ç†å†…å®¹: {content.content_id}")
        return processed_content
    
    def _preprocess_text(self, text: str, source_type: str) -> str:
        """é¢„å¤„ç†å•ä¸ªæ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            source_type: æ¥æºç±»å‹
            
        Returns:
            str: é¢„å¤„ç†åçš„æ–‡æœ¬
        """
        if not text or not text.strip():
            return ""
        
        # 1. HTMLè§£ç 
        text = html.unescape(text)
        
        # 2. ç§»é™¤HTMLæ ‡ç­¾
        text = self.html_pattern.sub('', text)
        
        # 3. ç§»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
        text = self.special_chars_pattern.sub('', text)
        
        # 4. å¤„ç†URLå’Œ@ç”¨æˆ·åï¼ˆæ ¹æ®æ¥æºç±»å‹å†³å®šæ˜¯å¦ä¿ç•™ï¼‰
        if source_type in ['title', 'body']:
            # æ­£æ–‡ä¸­çš„é“¾æ¥æ›¿æ¢ä¸ºå ä½ç¬¦
            text = self.url_pattern.sub('[é“¾æ¥]', text)
            text = self.mention_pattern.sub('[ç”¨æˆ·]', text)
        else:
            # OCR/ASRä¸­ç›´æ¥ç§»é™¤
            text = self.url_pattern.sub('', text)
            text = self.mention_pattern.sub('', text)
        
        # 5. å¤„ç†è¯é¢˜æ ‡ç­¾ï¼ˆä¿ç•™ä½†ç®€åŒ–ï¼‰
        text = self.hashtag_pattern.sub(lambda m: m.group(0)[1:], text)  # ç§»é™¤#å·
        
        # 6. å¤„ç†è¡¨æƒ…ç¬¦å·
        text = self._process_emojis(text)
        
        # 7. æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
        text = self._normalize_punctuation(text)
        
        # 8. æ¸…ç†å¤šä½™ç©ºç™½
        text = self.whitespace_pattern.sub(' ', text)
        text = text.strip()
        
        return text
    
    def _process_emojis(self, text: str) -> str:
        """å¤„ç†è¡¨æƒ…ç¬¦å·
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            str: å¤„ç†åçš„æ–‡æœ¬
        """
        # ä¿ç•™é‡è¦çš„è¡¨æƒ…ç¬¦å·
        important_emojis = self.emoji_keep_pattern.findall(text)
        
        # ç§»é™¤å…¶ä»–è¡¨æƒ…ç¬¦å·
        text = self.emoji_remove_pattern.sub('', text)
        
        # å°†é‡è¦è¡¨æƒ…ç¬¦å·è½¬æ¢ä¸ºæ–‡æœ¬æè¿°
        emoji_map = {
            'ğŸ‘': '[èµ]',
            'ğŸ‘': '[è¸©]', 
            'ğŸ’•': '[çˆ±å¿ƒ]',
            'â¤ï¸': '[çº¢å¿ƒ]',
            'ğŸ˜': '[èŠ±ç—´]',
            'ğŸ¥°': '[å¯çˆ±]',
            'ğŸ˜Š': '[å¾®ç¬‘]',
            'ğŸ˜­': '[å“­]',
            'ğŸ˜‚': '[ç¬‘å“­]',
            'ğŸ”¥': '[ç«]',
            'ğŸ’¯': '[æ»¡åˆ†]',
            'âœ¨': '[é—ªäº®]'
        }
        
        for emoji, desc in emoji_map.items():
            text = text.replace(emoji, desc)
        
        return text
    
    def _normalize_punctuation(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            str: æ ‡å‡†åŒ–åçš„æ–‡æœ¬
        """
        # ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        punctuation_map = {
            'ï¼ï¼+': 'ï¼',
            'ï¼Ÿï¼Ÿ+': 'ï¼Ÿ',
            'ã€‚ã€‚+': 'ã€‚',
            'ï¼Œï¼Œ+': 'ï¼Œ',
            'ï¼›ï¼›+': 'ï¼›',
            'ï¼šï¼š+': 'ï¼š',
            '~~~+': '~',
            '---+': '-'
        }
        
        for pattern, replacement in punctuation_map.items():
            text = re.sub(pattern, replacement, text)
        
        # å¤„ç†é‡å¤æ ‡ç‚¹
        text = self.repeat_punct_pattern.sub(r'\1', text)
        
        # æ ‡å‡†åŒ–å¼•å·
        text = re.sub(r'[""'']', '"', text)
        
        return text
    
    def _validate_processed_content(self, content: ContentInput):
        """éªŒè¯é¢„å¤„ç†åçš„å†…å®¹
        
        Args:
            content: é¢„å¤„ç†åçš„å†…å®¹
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
        has_content = any([
            content.title and content.title.strip(),
            content.body and content.body.strip(),
            content.ocr_text and content.ocr_text.strip(),
            content.asr_text and content.asr_text.strip()
        ])
        
        if not has_content:
            logger.warning(f"é¢„å¤„ç†åå†…å®¹ä¸ºç©º: {content.content_id}")
    
    def get_primary_text(self, content: ContentInput) -> Tuple[str, SourceType]:
        """è·å–ä¸»è¦æ–‡æœ¬å†…å®¹
        
        Args:
            content: å†…å®¹å¯¹è±¡
            
        Returns:
            Tuple[str, SourceType]: (ä¸»è¦æ–‡æœ¬, æ¥æºç±»å‹)
        """
        # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©ä¸»è¦æ–‡æœ¬
        for source in self.source_priority:
            if source == 'title' and content.title:
                return content.title, SourceType.TITLE
            elif source == 'body' and content.body:
                return content.body, SourceType.BODY
            elif source == 'asr' and content.asr_text:
                return content.asr_text, SourceType.ASR
            elif source == 'ocr' and content.ocr_text:
                return content.ocr_text, SourceType.OCR
        
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        return "", SourceType.BODY
    
    def segment_text(self, text: str, max_chars: Optional[int] = None) -> List[str]:
        """åˆ†æ®µæ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_chars: æœ€å¤§å­—ç¬¦æ•°
            
        Returns:
            List[str]: åˆ†æ®µåçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not text:
            return []
        
        max_chars = max_chars or self.max_chars_per_segment
        
        # å¦‚æœæ–‡æœ¬é•¿åº¦å°äºé™åˆ¶ï¼Œç›´æ¥è¿”å›
        if len(text) <= max_chars:
            return [text]
        
        segments = []
        current_pos = 0
        
        while current_pos < len(text):
            # è®¡ç®—å½“å‰æ®µçš„ç»“æŸä½ç½®
            end_pos = current_pos + max_chars
            
            if end_pos >= len(text):
                # æœ€åä¸€æ®µ
                segments.append(text[current_pos:])
                break
            
            # å¯»æ‰¾åˆé€‚çš„åˆ†å‰²ç‚¹ï¼ˆå¥å·ã€æ„Ÿå¹å·ã€é—®å·ï¼‰
            segment_text = text[current_pos:end_pos]
            
            # ä»åå¾€å‰æ‰¾æ ‡ç‚¹ç¬¦å·
            split_pos = -1
            for i in range(len(segment_text) - 1, -1, -1):
                if segment_text[i] in 'ã€‚ï¼ï¼Ÿ':
                    split_pos = i + 1
                    break
            
            if split_pos > 0 and split_pos > len(segment_text) * 0.5:
                # æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹
                segments.append(text[current_pos:current_pos + split_pos])
                current_pos += split_pos
            else:
                # æ²¡æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹ï¼Œå¼ºåˆ¶åˆ†å‰²
                segments.append(text[current_pos:end_pos])
                current_pos = end_pos
        
        return segments


class PreprocessingPipeline:
    """é¢„å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config = load_config(config_path) if config_path else {}
        self.preprocessor = TextPreprocessor(self.config)
        
    def process(self, contents: List[ContentInput]) -> List[ContentInput]:
        """å¤„ç†é¢„å¤„ç†"""
        logger.info(f"å¼€å§‹æ‰¹é‡é¢„å¤„ç†ï¼Œå…± {len(contents)} æ¡å†…å®¹")
        
        processed_contents = []
        
        for content in contents:
            try:
                processed_content = self.preprocessor.preprocess_content(content)
                processed_contents.append(processed_content)
                
            except Exception as e:
                logger.error(f"é¢„å¤„ç†å¤±è´¥ {content.content_id}: {e}")
                # ä¿ç•™åŸå§‹å†…å®¹
                processed_contents.append(content)
                
        logger.info(f"æ‰¹é‡é¢„å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(processed_contents)} æ¡")
        return processed_contents


# ä¾¿æ·å‡½æ•°
def create_preprocessing_pipeline(config_path: Optional[str] = None) -> PreprocessingPipeline:
    """åˆ›å»ºé¢„å¤„ç†æµæ°´çº¿"""
    return PreprocessingPipeline(config_path)


def preprocess_contents(
    contents: List[ContentInput],
    config_path: Optional[str] = None
) -> List[ContentInput]:
    """é¢„å¤„ç†å†…å®¹åˆ—è¡¨"""
    pipeline = create_preprocessing_pipeline(config_path)
    return pipeline.process(contents)